services:
  llama:
    command:
      - --model
      - /models/${LLM_GGUF:-Qwen3-4B-Q4_K_M.gguf}
      - --host
      - 0.0.0.0
      - --port
      - "8080"
      - --ctx-size
      - "${CTX_SIZE:-4096}"
      - --parallel
      - "${LLM_PARALLEL:-2}"
      - --cont-batching
      - --n-gpu-layers
      - "${GPU_LAYERS:--1}"
    dns:
      - ${DNS1:-1.1.1.1}
      - ${DNS2:-8.8.8.8}
