services:
  # === LLM Server (llama.cpp) ===
  llama:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: smartfarm-llama
    command:
      - --model
      - ${LLAMA_MODEL_PATH:-/models/Qwen3-4B-Q4_K_M.gguf}
      - --host
      - 0.0.0.0
      - --port
      - "8080"
      - --ctx-size
      - ${LLAMA_CTX_SIZE:-16384}
      - --parallel
      - ${LLAMA_PARALLEL:-2}
      - --cont-batching
      - --n-gpu-layers
      - ${GPU_LAYERS:--1}
    ports:
      - "${LLAMA_HOST_PORT:-45857}:8080"
    environment:
      - LLAMA_ARG_MAX_CONTEXT_SIZE=${LLAMA_ARG_MAX_CONTEXT_SIZE:-8192}
      - GPU_LAYERS=${GPU_LAYERS:--1}
    volumes:
      - ./models:/models
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    restart: unless-stopped
